# 12. Support Vector Machines (SVMs)

- 회귀, 분류, 이상치 탐지 등에 사용되는 지도학습 방식
- 클래스 사이의 경계에 위치한 데이터 포인트를 서포트 벡터(support vector)라고 함
- 각 서포트 벡터가 클래스 사이의 결정 경계를 구분하는데 얼마나 중요한지를 학습
- 각 서포트 벡터 사이의 마진이 가장 큰 방향으로 학습
- 서포트 벡터 까지의 거리와 지서포트지 벡터의 중요도를 기반으로 예측을 수행
- --
- sigmoid function의 cost가 아래와 같다고하자.
    - ![sigmoidFunction](http://www.holehouse.org/mlclass/12_Support_Vector_Machines_files/Image%20[4].png)
- 이때, y=1일 때 costfunction의 그래프는...
    - ![y=1,sigmoidCostFunction](http://www.holehouse.org/mlclass/12_Support_Vector_Machines_files/Image%20[5].png)
- 이때, y=0일 때 costfunction의 그래프는...
    - ![y=0,sigmoidCostFunction](http://www.holehouse.org/mlclass/12_Support_Vector_Machines_files/Image%20[6].png)
### SVM cost functions from logistic regression cost function
- y=1일 때, 자홍색 선으로 cost function 표시
    - ![y=1,SVM](http://www.holehouse.org/mlclass/12_Support_Vector_Machines_files/Image%20[7].png)
- y=1일 때, 자홍색 선으로 cost function 표시
    - ![y=0,SVM](http://www.holehouse.org/mlclass/12_Support_Vector_Machines_files/Image%20[8].png)
### complete SVM cost function
- SVM cost function 식
    - ![SVMcostFunction](http://www.holehouse.org/mlclass/12_Support_Vector_Machines_files/Image%20[10].png)
    
### SVM notation는 조금 달라!
1. 1/m을 지움
    - 1/m을 지워도 같은 optimal value를 얻어야 됨
    - 1/m은 상수니까, 똑같은 optimiztion을 가짐!
        - 1 2 3 4 에 다 5곱해도 맨 앞이 가장 작은건 똑같음
2. CA+B
    - logistic regression에서는 a+&lambda;b형식으로 표시했지만 SVM은 아님
    - 우리는 C라고 불리는 다른 parameter를 사용하여 표기할거야
    - CA+B 이런 식으로
    - 만약 C가 1/&lambda;와 같으면 CA+B랑 a+&lambda;b랑 같은 값을 줄거양
    - ![finalEquation](http://www.holehouse.org/mlclass/12_Support_Vector_Machines_files/Image%20[11].png)
- logistic과 달리 h<sub>&theta;</sub>는 확률을 안줘... 하지만 1또는 0이라는 예측을 바로 얻을 수 있지!
    - 만약 &Theta;<sup>T</sup>x가 0보다 크면~ h<sub>&theta;</sub>는 1, 아니면 0

## Large margin intuition
- 