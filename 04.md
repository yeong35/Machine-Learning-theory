# 04: Linear Regression with Multiple Variables

## Linear regression with muliple features
- Multiple variables = multiple features
- 만약, 새 sheme에서 다양한 변수들이 주어진다면...(4개의 변수를 갖는다고 가정하자.)
    - x<sub>1</sub> - size(feet squared)
    - x<sub>2</sub> - Number of bedrooms
    - x<sub>3</sub> - Number of floors
    - x<sub>4</sub> - Age of home(years)
    - y - output variale(price)
- More notation
    - n - number of features
    - m - number of examples
    - x<sup>i</sup> - vector of the input for an example
    - x<sub>j</sub><sup>i</sup> - The value of feature j in the *i*th training example
- h<sub>&theta;</sub>(x)=&theta;<sub>0</sub>+&theta;<sub>1</sub>x<sub>1</sub>+&theta;<sub>2</sub>x<sub>2</sub>+&theta;<sub>3</sub>x<sub>3</sub>+&theta;<sub>4</sub>x<sub>4</sub>
- h<sub>&theta;</sub>(x)=&theta;<sup>T</sup>X 로 표현이 가능하다.
    - &theta;는 열벡터, &theta;<sup>T</sup>는 행벡터


## Gradient descent for multiple variables
- Parameter가 0부터 n까지 있을 때, vector(&theta;)라고 생각하자. &theta;는 [n+1 x 1] 벡터
- cost function은...
    - <a href="https://www.codecogs.com/eqnedit.php?latex=J(\Theta_0&space;,\Theta&space;_1&space;,&space;...,&space;\Theta&space;_n&space;)&space;=\frac{1}{2m}\sum_{i=1}^{m}(h_\Theta&space;(x^{(i)})-y^{(i)}&space;)^2" target="_blank"><img src="https://latex.codecogs.com/gif.latex?J(\Theta_0&space;,\Theta&space;_1&space;,&space;...,&space;\Theta&space;_n&space;)&space;=\frac{1}{2m}\sum_{i=1}^{m}(h_\Theta&space;(x^{(i)})-y^{(i)}&space;)^2" title="J(\Theta_0 ,\Theta _1 , ..., \Theta _n ) =\frac{1}{2m}\sum_{i=1}^{m}(h_\Theta (x^{(i)})-y^{(i)} )^2" /></a>
- Gradient descent는
    - <a href="https://www.codecogs.com/eqnedit.php?latex=\Theta&space;_j&space;:=&space;\Theta&space;_j&space;-&space;\alpha&space;\frac{1}{m}\sum_{i=1}^{m}(h_\Theta&space;(x^{(i)})-y^{(i)}&space;)x^{(i)}_{j}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\Theta&space;_j&space;:=&space;\Theta&space;_j&space;-&space;\alpha&space;\frac{1}{m}\sum_{i=1}^{m}(h_\Theta&space;(x^{(i)})-y^{(i)}&space;)x^{(i)}_{j}" title="\Theta _j := \Theta _j - \alpha \frac{1}{m}\sum_{i=1}^{m}(h_\Theta (x^{(i)})-y^{(i)} )x^{(i)}_{j}" /></a>
    - 위를 (0부터 n까지 동시에) 반복하는 것이다!
- 다른 말로는...
    - <a href="https://www.codecogs.com/eqnedit.php?latex=\Theta&space;_j&space;:=&space;\Theta&space;_j&space;-&space;\alpha&space;\frac{d}{d\theta_j&space;}J(\theta)" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\Theta&space;_j&space;:=&space;\Theta&space;_j&space;-&space;\alpha&space;\frac{d}{d\theta_j&space;}J(\theta)" title="\Theta _j := \Theta _j - \alpha \frac{d}{d\theta_j }J(\theta)" /></a>
- 아래의 식을 잘 기억해두자
    - <a href="https://www.codecogs.com/eqnedit.php?latex=\frac{1}{m}\sum_{i=1}^{m}(h_\Theta&space;(x^{(i)})-y^{(i)}&space;)x^{(i)}_{j}&space;=&space;\frac{d}{d\Theta&space;_j&space;}J(\Theta&space;)" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\frac{1}{m}\sum_{i=1}^{m}(h_\Theta&space;(x^{(i)})-y^{(i)}&space;)x^{(i)}_{j}&space;=&space;\frac{d}{d\Theta&space;_j&space;}J(\Theta&space;)" title="\frac{1}{m}\sum_{i=1}^{m}(h_\Theta (x^{(i)})-y^{(i)} )x^{(i)}_{j} = \frac{d}{d\Theta _j }J(\Theta )" /></a>
## Gradient Decent in practice: 1 Feature Scaling // 이해 안됨
- feature들을 비슷한 scale로 만들어주면? -> gradient descent가 더 빨리 minimum에 도달할 수 있다
- 

## Learning Rate &alpha;  
- learning rate &alpha;에 대해 알아보자
    - update rule
    - debugging
    - 어떻게 alpha를 정하는 가?
- gradient descent가 제대로 작동한다면, J(&theta;)는 실행할때마다 감소할 것이다
- learning rate가 너무 크다면? -> J(&theta;)가 감소하지 않을 수 있다
- learning rate가 너무 작다면? -> 각 실행마다 조금씩 감소함
- 다양한 alpha값을 사용하는 것이 좋음
- alpha값을 **3배씩 증가시켜보자**
    - ex) 0.001, 0.003, 0.01, ..., 0.3
    
## Features and polynomial regression

## Normal equation

## Normal equation and non-invertibility
