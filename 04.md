# 04: Linear Regression with Multiple Variables

## Linear regression with muliple features
- Multiple variables = multiple features
- 만약, 새 sheme에서 다양한 변수들이 주어진다면...(4개의 변수를 갖는다고 가정하자.)
    - x<sub>1</sub> - size(feet squared)
    - x<sub>2</sub> - Number of bedrooms
    - x<sub>3</sub> - Number of floors
    - x<sub>4</sub> - Age of home(years)
    - y - output variale(price)
- More notation
    - n - number of features
    - m - number of examples
    - x<sup>i</sup> - vector of the input for an example
    - x<sub>j</sub><sup>i</sup> - The value of feature j in the *i*th training example
- h<sub>&theta;</sub>(x)=&theta;<sub>0</sub>+&theta;<sub>1</sub>x<sub>1</sub>+&theta;<sub>2</sub>x<sub>2</sub>+&theta;<sub>3</sub>x<sub>3</sub>+&theta;<sub>4</sub>x<sub>4</sub>
- h<sub>&theta;</sub>(x)=&theta;<sup>T</sup>X 로 표현이 가능하다.
    - &theta;는 열벡터, &theta;<sup>T</sup>는 행벡터
## Gradient descent for multiple variables
- Parameter가 0부터 n까지 있을 때, vector(&theta;)라고 생각하자. &theta;는 [n+1 x 1] 벡터
- <a href="https://www.codecogs.com/eqnedit.php?latex=J(\Theta_0&space;,\Theta&space;_1&space;,&space;...,&space;\Theta&space;_n&space;)&space;=\frac{1}{2m}\sum_{i=1}^{m}(h_\Theta&space;(x^{(i)})-y^{(i)}&space;)^2" target="_blank"><img src="https://latex.codecogs.com/gif.latex?J(\Theta_0&space;,\Theta&space;_1&space;,&space;...,&space;\Theta&space;_n&space;)&space;=\frac{1}{2m}\sum_{i=1}^{m}(h_\Theta&space;(x^{(i)})-y^{(i)}&space;)^2" title="J(\Theta_0 ,\Theta _1 , ..., \Theta _n ) =\frac{1}{2m}\sum_{i=1}^{m}(h_\Theta (x^{(i)})-y^{(i)} )^2" /></a>
- 
## Gradient Decent in practice: 1 Feature Scaling

## Learning Rate &alpha;

## Features and polynomial regression

## Normal equation

## Normal equation and non-invertibility
